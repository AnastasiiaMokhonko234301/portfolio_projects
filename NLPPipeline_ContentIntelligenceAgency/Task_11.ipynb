{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c166cd6",
   "metadata": {},
   "source": [
    "# Full Emotion Detection Pipeline\n",
    "\n",
    "This pipeline takes raw video input, transcribes the audio detected, translates the transcription sentences into English using a pretrained OPUS model, and uses a CamemBERT model to analyze emotions detected on the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce492c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer, AutoModel, MarianMTModel, MarianTokenizer, CamembertForSequenceClassification, CamembertTokenizer, pipeline, Trainer, TrainingArguments, get_scheduler, TFAutoModelForSeq2SeqLM\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import json\n",
    "import assemblyai as aai\n",
    "import re\n",
    "import spacy\n",
    "from textblob import TextBlob, Blobber\n",
    "from textblob_fr import PatternAnalyzer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset as HFDataset, load_dataset\n",
    "import evaluate\n",
    "import nlpaug.augmenter.word as naw\n",
    "from googletrans import Translator\n",
    "import time\n",
    "from textattack.augmentation import EasyDataAugmenter\n",
    "from IPython.display import display\n",
    "\n",
    "# Ensure NLTK resources are available (run this once if you haven't before)\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    from textblob_fr import PatternAnalyzer\n",
    "except ImportError:\n",
    "    print(\"Warning: textblob_fr might not be installed correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8888a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Extract audio from a video file\n",
    "\n",
    "aai.settings.api_key = \"b5e1d69d7be14bb291bff901bf4dc1c8\"\n",
    "\n",
    "audio_file = \"/Users/daria/Desktop/2024-25c-fai2-adsai-dariavladutu236578/Week_1/extracted mp3s/Le Schtroumpf Navigateur â€¢ Les Schtroumpfs.mp3\"\n",
    "\n",
    "# Initialize transcriber with French language config\n",
    "config = aai.TranscriptionConfig(language_code=\"fr\")\n",
    "transcriber = aai.Transcriber()\n",
    "\n",
    "# Start transcription\n",
    "transcript_id = transcriber.transcribe(audio_file, config=config)\n",
    "\n",
    "# Wait for completion\n",
    "transcript = transcriber.wait_for_completion(transcript_id)\n",
    "\n",
    "if transcript.status == aai.TranscriptStatus.error:\n",
    "    print(f\"Transcription failed: {transcript.error}\")\n",
    "    exit(1)\n",
    "#print(transcript.text)\n",
    "# Extract sentences (assuming you want individual sentences in rows)\n",
    "sentences = [{\"Sentence\": s.strip()} for s in re.split(r'(?<=[.!?]) +', transcript.text) if s.strip()]\n",
    "\n",
    "# Convert to a DataFrame\n",
    "df = pd.DataFrame(sentences)\n",
    "\n",
    "# Specify output Excel file name\n",
    "excel_filename = \"Le_Schtroumpf_Navigateur_Les_Schtroumpfs.xlsx\"\n",
    "\n",
    "# Save to an Excel file\n",
    "df.to_excel(excel_filename, index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(f\"Transcription saved as {excel_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17720314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Tokenize and calculate Word Error Rate (WER) for transcripts\n",
    "\n",
    "# Load SpaCy French language model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# File paths\n",
    "file_paths = {\n",
    "    \"assemblyAI\": r\"Week_1/STT_Assembly.xlsx\",\n",
    "    \"whisper\": r\"Week_1/STT_Whisper.xlsx\"\n",
    "}\n",
    "\n",
    "# Function to tokenize text using SpaCy\n",
    "def tokenize_french(text):\n",
    "    doc = nlp(text.lower())  # Process text and lowercase\n",
    "    tokens = [token.text for token in doc if not token.is_punct and not token.is_space]  # Remove punctuation & spaces\n",
    "    return tokens\n",
    "\n",
    "# Function to calculate Word Error Rate (WER)\n",
    "def calculate_wer(df, token_column, s_col='S', i_col='I', d_col='D'):\n",
    "    if s_col in df.columns and i_col in df.columns and d_col in df.columns:\n",
    "        S = df[s_col].sum()\n",
    "        I = df[i_col].sum()\n",
    "        D = df[d_col].sum()\n",
    "        N = df[token_column].explode().count()  # Total token count\n",
    "        WER = (S + I + D) / N if N > 0 else 0\n",
    "        return WER\n",
    "    else:\n",
    "        raise ValueError(\"Missing S, I, or D columns in the file.\")\n",
    "\n",
    "# Function to count tokens and compute WER\n",
    "def process_transcript(file_path, header_row=0):\n",
    "    df = pd.read_excel(file_path, header=header_row)\n",
    "    \n",
    "    # Identify the transcription column\n",
    "    transcription_col = next((col for col in df.columns if \"sentence\" in col.lower() or \"transcription\" in col.lower()), None)\n",
    "\n",
    "    if transcription_col is None:\n",
    "        raise ValueError(f\"No transcription column found in {file_path}.\")\n",
    "    \n",
    "    # Tokenize and count tokens\n",
    "    df['tokens'] = df[transcription_col].astype(str).apply(tokenize_french)\n",
    "    total_tokens = df['tokens'].explode().count()\n",
    "    \n",
    "    # Calculate WER\n",
    "    wer_score = calculate_wer(df, 'tokens')\n",
    "    \n",
    "    return df, total_tokens, wer_score\n",
    "\n",
    "# Process both transcripts and compute WER\n",
    "dfs = {}\n",
    "wer_scores = {}\n",
    "\n",
    "for model, path in file_paths.items():\n",
    "    df, total_tokens, wer = process_transcript(path, header_row=0)\n",
    "    dfs[model] = df\n",
    "    wer_scores[model] = wer\n",
    "    print(f\"Total tokens in {model} transcript: {total_tokens}\")\n",
    "    print(f\"WER for {model} transcript: {wer:.4f}\")\n",
    "\n",
    "## OUTPUT\n",
    "# Total tokens in assemblyAI transcript: 1263\n",
    "# WER for assemblyAI transcript: 0.0507\n",
    "# Total tokens in whisper transcript: 1434\n",
    "# WER for whisper transcript: 0.0704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b21794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Preprocessing the data \n",
    "# Load the data \n",
    "# Adding the Emotion detection dataset (with phrases from the Friends show)\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load JSON file\n",
    "with open(\"emotion-detection-emotion-detection-1.0/json/emotion-detection-trn.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "dataf = pd.DataFrame(data)\n",
    "\n",
    "# Expand the 'episodes' column\n",
    "episodes_list = []\n",
    "\n",
    "for _, row in dataf.iterrows():\n",
    "    season_id = row[\"season_id\"]  # Track season ID\n",
    "\n",
    "    # Check if the episodes column is a dictionary and process accordingly\n",
    "    if isinstance(row['episodes'], dict):  # If episodes is already a dict\n",
    "        episodes = [row['episodes']]  # Put it in a list for uniform processing\n",
    "    elif isinstance(row['episodes'], str):  # If episodes is a string, try parsing it\n",
    "        try:\n",
    "            episodes = json.loads(row['episodes'])  # Convert string to dictionary\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON for row {row.name}\")\n",
    "            continue\n",
    "    else:\n",
    "        episodes = []  # Empty list if structure is unexpected\n",
    "\n",
    "    # Iterate through the episodes\n",
    "    for episode in episodes:\n",
    "        # Make sure episode is a dictionary and has the expected keys\n",
    "        if isinstance(episode, dict) and \"episode_id\" in episode and \"scenes\" in episode:\n",
    "            episode_id = episode[\"episode_id\"]\n",
    "            for scene in episode[\"scenes\"]:\n",
    "                # Make sure each scene has the expected structure\n",
    "                if isinstance(scene, dict) and \"scene_id\" in scene and \"utterances\" in scene:\n",
    "                    scene_id = scene[\"scene_id\"]\n",
    "                    for utterance in scene[\"utterances\"]:\n",
    "                        if isinstance(utterance, dict) and \"utterance_id\" in utterance:\n",
    "                            episodes_list.append({\n",
    "                                \"season_id\": season_id,\n",
    "                                \"episode_id\": episode_id,\n",
    "                                \"scene_id\": scene_id,\n",
    "                                \"utterance_id\": utterance[\"utterance_id\"],\n",
    "                                \"speaker\": utterance[\"speakers\"][0] if utterance[\"speakers\"] else None,\n",
    "                                \"transcript\": utterance[\"transcript\"],\n",
    "                                \"emotion\": utterance[\"emotion\"]\n",
    "                            })\n",
    "        else:\n",
    "            print(f\"Unexpected episode structure: {episode}\")\n",
    "\n",
    "# Create a new DataFrame\n",
    "df = pd.DataFrame(episodes_list)\n",
    "\n",
    "# Copy the dataset in case we need to restart the preprocessing\n",
    "df_copy = df.copy()\n",
    "\n",
    "# Remove the unnecessary columns\n",
    "columns_to_keep = [\"transcript\", \"emotion\"]\n",
    "df_copy = df_copy[columns_to_keep]\n",
    "\n",
    "# Replace their 7 labels ('Joyful' 'Neutral' 'Powerful' 'Mad' 'Sad' 'Scared' 'Peaceful') \n",
    "# with our 7 labels (\"happiness\", \"sadness\", \"anger\", \"surprise\", \"fear\", \"disgust\", \"neutral\")\n",
    "df_copy.loc[:, \"emotion\"] = df_copy[\"emotion\"].str.replace(\"Joyful\", \"happiness\")\n",
    "df_copy.loc[:, \"emotion\"] = df_copy[\"emotion\"].str.replace(\"Neutral\", \"neutral\")\n",
    "df_copy.loc[:, \"emotion\"] = df_copy[\"emotion\"].str.replace(\"Powerful\", \"happiness\")\n",
    "df_copy.loc[:, \"emotion\"] = df_copy[\"emotion\"].str.replace(\"Mad\", \"anger\")\n",
    "df_copy.loc[:, \"emotion\"] = df_copy[\"emotion\"].str.replace(\"Sad\", \"sadness\")\n",
    "df_copy.loc[:, \"emotion\"] = df_copy[\"emotion\"].str.replace(\"Scared\", \"fear\")\n",
    "df_copy.loc[:, \"emotion\"] = df_copy[\"emotion\"].str.replace(\"Peaceful\", \"happiness\")\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "def translate_function(text):\n",
    "    try:\n",
    "        print(f\"Translating: {text}\")  # Add this to see if the function is being called\n",
    "        if not isinstance(text, str) or text.strip() == \"\":\n",
    "            print(f\"Skipping invalid input: {text}\")\n",
    "            return text\n",
    "        \n",
    "        time.sleep(0.5)  # Avoid rate limits\n",
    "        translated_text = translator.translate(text, dest=\"fr\").text  # This should be synchronous\n",
    "        \n",
    "        if not translated_text:\n",
    "            print(f\"Empty translation for: {text}\")\n",
    "            return text  \n",
    "\n",
    "        return translated_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error translating: {text} - {e}\")\n",
    "        return text\n",
    "\n",
    "# Apply the translation function to the \"Sentence\" column\n",
    "df_copy[\"Translated\"] = df_copy[\"Sentence\"].apply(translate_function)\n",
    "\n",
    "# Rename the text column to resemble our transcribed dataset better\n",
    "df_copy.rename(columns={\"emotion\": \"Emotion\", \"transcript\": \"Sentence\"}, inplace=True)\n",
    "df2 = df_copy\n",
    "\n",
    "# Switching the column names to match the final dataset\n",
    "df2.rename(columns={'Translated': 'Sentence', 'Sentence': 'Translated'}, inplace=True)\n",
    "df2 = df2[['Sentence'] + [col for col in df2.columns if col != 'Sentence']]\n",
    "\n",
    "# Adding synthetic data to balance classes with the lowest nr of instances (sadness, surprise, and disgust)\n",
    "transcript_3 = \"synthetic_emotion_dataset.csv\"\n",
    "df3 = pd.read_csv(transcript_3)\n",
    "\n",
    "# Adding the Go Emotions dataset\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/sebdg/go_emotions_cleaned/\" + splits[\"train\"])\n",
    "\n",
    "# Copy the dataset in case we need to restart the preprocessing\n",
    "df_copy = df\n",
    "df.dropna()\n",
    "\n",
    "# Remove the unnecessary columns\n",
    "columns_to_keep = [\"text\", \"labels\", \"labels_text\"]\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Split the labels_text column into multiple\n",
    "df = df.assign(labels_text=df[\"labels_text\"].str.split(\",\")).explode(\"labels_text\")\n",
    "display(df.tail(10))\n",
    "\n",
    "# Make all label names lowercase\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.strip().str.lower()\n",
    "\n",
    "# Replace the 28 ('admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', \n",
    "# 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', \n",
    "# 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral') labels \n",
    "# with our 7 labels (\"happiness\", \"sadness\", \"anger\", \"surprise\", \"fear\", \"disgust\", \"neutral\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"admiration\", \"happiness\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"amusement\", \"happiness\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"anger\", \"anger\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"annoyance\", \"anger\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"approval\", \"happiness\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"caring\", \"happiness\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"confusion\", \"surprise\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"curiosity\", \"surprise\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"desire\", \"happiness\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"disappointment\", \"disgust\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"disapproval\", \"disgust\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"disgust\", \"disgust\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"embarrassment\", \"sadness\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"excitement\", \"happiness\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"fear\", \"fear\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"gratitude\", \"happiness\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"grief\", \"sadness\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"joy\", \"happiness\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"love\", \"happiness\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"nervousness\", \"fear\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"optimism\", \"happiness\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"pride\", \"happiness\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"realization\", \"surprise\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"relief\", \"happiness\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"remorse\", \"sadness\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"sadness\", \"sadness\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"surprise\", \"surprise\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"neutral\", \"neutral\")\n",
    "df[\"labels_text\"] = df[\"labels_text\"].str.replace(\"dishappiness\", \"sadness\")\n",
    "\n",
    "# Rename the text column to resemble our transcribed dataset better\n",
    "df.rename(columns={\"text\": \"Translated\"}, inplace=True)\n",
    "\n",
    "# Drop duplicates\n",
    "df = df.drop_duplicates(subset=[\"Translated\"], keep=\"first\")\n",
    "\n",
    "# Translate\n",
    "# Check if CUDA is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    " \n",
    "# Optimize PyTorch for GPU\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    " \n",
    "# Load model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)  # Move model to GPU\n",
    " \n",
    "# Warm-up GPU to avoid initial slow batch\n",
    "dummy_input = torch.tensor([[0]]).to(device)  \n",
    " \n",
    "# Function to translate sentences in batches\n",
    "def translate(sentences, batch_size=8):  # Reduce batch size if stuck\n",
    "    translated = []\n",
    "    for i in tqdm(range(0, len(sentences), batch_size), desc=\"Translating\", unit=\"batch\"):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "       \n",
    "        # Tokenize & move input to GPU\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "       \n",
    "        # Generate translation using GPU\n",
    "        with torch.no_grad():\n",
    "            translated_batch = model.generate(**inputs)\n",
    "       \n",
    "        # Decode output\n",
    "        translated.extend([tokenizer.decode(t, skip_special_tokens=True) for t in translated_batch])\n",
    "   \n",
    "    return translated\n",
    " \n",
    "# Translate sentences\n",
    "df['Sentence'] = translate(df['Translated'].tolist())\n",
    "\n",
    "# Move sentence column first\n",
    "df = df[['Sentence'] + [col for col in df.columns if col != 'Sentence']]\n",
    "df.drop(columns=['labels'], inplace=True)\n",
    "df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "# Drop encoded labels column\n",
    "df.drop(columns=['labels'], inplace=True)\n",
    "df.rename(columns = {'labels_text':'Emotion'})\n",
    "\n",
    "df4 = df\n",
    "\n",
    "# Concatanate datasets\n",
    "df = pd.concat([df2, df3, df4], ignore_index=True)\n",
    "display(df)\n",
    "\n",
    "# Addind synthetic data to balance the classes\n",
    "# Initialize TextAttack augmenter (you can experiment with different ones)\n",
    "augmenter = EasyDataAugmenter(pct_words_to_swap=0.2, transformations_per_example=2)\n",
    "\n",
    "# Define a threshold: Augment only classes below this threshold\n",
    "THRESHOLD = 9000\n",
    "\n",
    "# Apply augmentation to underrepresented classes\n",
    "augmented_sentences = []\n",
    "augmented_labels = []\n",
    "class_counts = df[\"Emotion\"].value_counts()\n",
    "for emotion, count in class_counts.items():\n",
    "    if count < THRESHOLD:\n",
    "        subset = df[df[\"Emotion\"] == emotion]\n",
    "        for sentence in subset[\"Sentence\"]:\n",
    "            augmented = augmenter.augment(sentence)\n",
    "            augmented_sentences.extend(augmented)\n",
    "            augmented_labels.extend([emotion] * len(augmented))\n",
    "\n",
    "# Create DataFrame for augmented data\n",
    "augmented_df = pd.DataFrame({\"Sentence\": augmented_sentences, \"Emotion\": augmented_labels})\n",
    "\n",
    "# Combine with original data\n",
    "df = pd.concat([df, augmented_df]).reset_index(drop=True)\n",
    "df = df[['Sentence', 'Emotion']]\n",
    "\n",
    "df1 = df\n",
    "# Add emotion pipeline output dataset, augment it with part of the other dataset\n",
    "tqdm.pandas()\n",
    "\n",
    "# Initialize French contextual augmenter (CamemBERT)\n",
    "aug = naw.ContextualWordEmbsAug(\n",
    "    model_path='camembert-base',\n",
    "    action=\"substitute\",\n",
    "    device='cuda'  # Use 'cpu' if no GPU\n",
    ")\n",
    "\n",
    "# Set desired target for class size\n",
    "TARGET_SIZE = 5000\n",
    "\n",
    "# Initialize a list to hold augmented sentences\n",
    "augmented_sentences = []\n",
    "augmented_labels = []\n",
    "\n",
    "# Iterate over each class and augment or sample\n",
    "for emotion, count in df1['Emotion'].value_counts().items():\n",
    "    print(f\"Processing {emotion} class with {count} samples...\")\n",
    "\n",
    "    # Subset the data for the current emotion\n",
    "    subset = df1[df1[\"Emotion\"] == emotion]\n",
    "\n",
    "    # If the class is 'fear', leave it untouched\n",
    "    if emotion == 'fear':\n",
    "        augmented_sentences.extend(subset[\"Sentence\"].tolist())\n",
    "        augmented_labels.extend([emotion] * len(subset))\n",
    "\n",
    "    # If the class size is greater than TARGET_SIZE, sample down to TARGET_SIZE\n",
    "    elif count > TARGET_SIZE:\n",
    "        print(f\"Sampling {emotion} class down to {TARGET_SIZE} samples...\")\n",
    "        augmented_sentences.extend(subset.sample(n=TARGET_SIZE, replace=False)[\"Sentence\"].tolist())\n",
    "        augmented_labels.extend([emotion] * TARGET_SIZE)\n",
    "\n",
    "    # If the class size is less than TARGET_SIZE, augment it\n",
    "    else:\n",
    "        print(f\"Augmenting {emotion} class...\")\n",
    "        augmented = subset[\"Sentence\"].progress_apply(lambda x: aug.augment(x, n=2)[0]).tolist()\n",
    "        \n",
    "        # Collect augmented sentences and labels\n",
    "        augmented_sentences.extend(augmented)\n",
    "        augmented_labels.extend([emotion] * len(augmented))\n",
    "\n",
    "# Create a DataFrame for the augmented data\n",
    "augmented_df = pd.DataFrame({\"Sentence\": augmented_sentences, \"Emotion\": augmented_labels})\n",
    "\n",
    "# Check the new class distribution\n",
    "df_final = augmented_df.groupby(\"Emotion\").apply(\n",
    "    lambda x: x.sample(n=TARGET_SIZE, replace=False) if len(x) > TARGET_SIZE else x\n",
    ").reset_index(drop=True)\n",
    "\n",
    "df1=df_final\n",
    "\n",
    "# Load pipeline output dataset\n",
    "df2 = pd.read_csv('test_data/group 12_url1.csv')\n",
    "\n",
    "# Keep only relevant columns\n",
    "df2.drop(columns=['Start Time', 'End Time', 'Sentence', 'Translation', 'Intensity'], inplace=True)\n",
    "\n",
    "df2.rename(columns={'Corrected Sentence': 'Sentence'}, inplace=True)\n",
    "\n",
    "# Rename emotions to fit with our labels\n",
    "df2[\"Emotion\"] = df2[\"Emotion\"].str.replace(\"excitement\", \"happiness\")\n",
    "df2[\"Emotion\"] = df2[\"Emotion\"].str.replace(\"confusion\", \"surprise\")\n",
    "df2[\"Emotion\"] = df2[\"Emotion\"].str.replace(\"annoyance\", \"anger\")\n",
    "df2[\"Emotion\"] = df2[\"Emotion\"].str.replace(\"disapproval\", \"disgust\")\n",
    "df2[\"Emotion\"] = df2[\"Emotion\"].str.replace(\"pride\", \"happiness\")\n",
    "df2[\"Emotion\"] = df2[\"Emotion\"].str.replace(\"joy\", \"happiness\")\n",
    "df2[\"Emotion\"] = df2[\"Emotion\"].str.replace(\"disappointment\", \"sadness\")\n",
    "df2[\"Emotion\"] = df2[\"Emotion\"].str.replace(\"optimism\", \"happiness\")\n",
    "df2[\"Emotion\"] = df2[\"Emotion\"].str.replace(\"admiration\", \"happiness\")\n",
    "df2[\"Emotion\"] = df2[\"Emotion\"].str.replace(\"approval\", \"happiness\")\n",
    "df2[\"Emotion\"] = df2[\"Emotion\"].str.replace(\"nervousness\", \"fear\")\n",
    "df2[\"Emotion\"] = df2[\"Emotion\"].str.replace(\"realization\", \"surprise\")\n",
    "df2[\"Emotion\"] = df2[\"Emotion\"].str.replace(\"gratitude\", \"happiness\")\n",
    "df2[\"Emotion\"] = df2[\"Emotion\"].str.replace(\"caring\", \"happiness\")\n",
    "df2[\"Emotion\"] = df2[\"Emotion\"].str.replace(\"love\", \"happiness\")\n",
    "df2[\"Emotion\"] = df2[\"Emotion\"].str.replace(\"remorse\", \"sadness\")\n",
    "df2[\"Emotion\"] = df2[\"Emotion\"].str.replace(\"embarrassment\", \"fear\")\n",
    "df2[\"Emotion\"] = df2[\"Emotion\"].str.replace(\"grief\", \"sadness\")\n",
    "\n",
    "# Remove unecessary rows that add to the 'happiness' class (there are enough samples)\n",
    "df2 = df2[~df2['Emotion'].isin(['curiosity', 'desire', 'relief','amusement'])]\n",
    "\n",
    "# Load your dataset (adjust path and column names if needed)\n",
    "tqdm.pandas()\n",
    "\n",
    "# Initialize French contextual augmenter (CamemBERT)\n",
    "aug = naw.ContextualWordEmbsAug(\n",
    "    model_path='camembert-base',\n",
    "    action=\"substitute\",\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "# Set desired target for class size\n",
    "TARGET_SIZE = 5000\n",
    "\n",
    "# Initialize a list to hold augmented sentences\n",
    "augmented_sentences = []\n",
    "augmented_labels = []\n",
    "\n",
    "# Iterate over each class and augment\n",
    "for emotion, count in df2['Emotion'].value_counts().items():\n",
    "    print(f\"Processing {emotion} class with {count} samples...\")\n",
    "\n",
    "    # Subset the data for the current emotion\n",
    "    subset = df2[df2[\"Emotion\"] == emotion]\n",
    "\n",
    "    # If class size is less than the target, augment it\n",
    "    if count < TARGET_SIZE:\n",
    "        print(f\"Augmenting {emotion} class...\")\n",
    "        # Augment each sentence in the subset\n",
    "        augmented = subset[\"Sentence\"].progress_apply(lambda x: aug.augment(x, n=2)[0]).tolist()\n",
    "        \n",
    "        # Collect augmented sentences and labels\n",
    "        augmented_sentences.extend(augmented)\n",
    "        augmented_labels.extend([emotion] * len(augmented))\n",
    "\n",
    "    # If the class is already above the target size, just take the original samples\n",
    "    else:\n",
    "        augmented_sentences.extend(subset[\"Sentence\"].tolist())\n",
    "        augmented_labels.extend([emotion] * len(subset))\n",
    "\n",
    "# Create a DataFrame for the augmented data\n",
    "augmented_df = pd.DataFrame({\"Sentence\": augmented_sentences, \"Emotion\": augmented_labels})\n",
    "\n",
    "# If there are still more than 5000 examples for a class, randomly sample to get exactly 5000\n",
    "df_final = augmented_df.groupby(\"Emotion\").apply(\n",
    "    lambda x: x.sample(n=TARGET_SIZE, replace=True) if len(x) < TARGET_SIZE else x.sample(n=TARGET_SIZE, replace=False)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Check the new class distribution\n",
    "print(\"After Augmentation:\\n\", df_final[\"Emotion\"].value_counts())\n",
    "\n",
    "df2 = pd.concat([df2, df_final], ignore_index=True)\n",
    "df = pd.concat([df1,df2], ignore_index=True)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "####\n",
    "# Test data preprocessing\n",
    "df1 = pd.read_csv('group 12_url1.csv')\n",
    "df2 = pd.read_csv('group 10_url1.csv')\n",
    "df1 = pd.concat([df1,df2], ignore_index=True)\n",
    "\n",
    "# Drop and rename columns\n",
    "df1.drop(columns=['Start Time', 'End Time', 'Sentence'], inplace=True)\n",
    "df1.rename(columns={'Corrected Sentence': 'Sentence'}, inplace=True)\n",
    "\n",
    "# Add another dataset\n",
    "df2 = pd.read_csv('la_villa_dataset.csv')\n",
    "\n",
    "# Capitalize column names\n",
    "df2.columns = [col.capitalize() for col in df1.columns]\n",
    "\n",
    "# Added synthethic dataset\n",
    "df3 = pd.read_csv('french_reality_tv_dataset.csv')\n",
    "df3.rename(columns={'Sentence (French)':'Sentence', 'Translation (English)': 'Translation'},inplace=True)\n",
    "\n",
    "# Concatanate \n",
    "df = pd.concat([df1,df2,df3])\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Rename emotions to fit with our labels\n",
    "df[\"Emotion\"] = df[\"Emotion\"].str.replace(\"excitement\", \"happiness\")\n",
    "df[\"Emotion\"] = df[\"Emotion\"].str.replace(\"confusion\", \"surprise\")\n",
    "df[\"Emotion\"] = df[\"Emotion\"].str.replace(\"annoyance\", \"anger\")\n",
    "df[\"Emotion\"] = df[\"Emotion\"].str.replace(\"disapproval\", \"disgust\")\n",
    "df[\"Emotion\"] = df[\"Emotion\"].str.replace(\"pride\", \"happiness\")\n",
    "df[\"Emotion\"] = df[\"Emotion\"].str.replace(\"joy\", \"happiness\")\n",
    "df[\"Emotion\"] = df[\"Emotion\"].str.replace(\"disappointment\", \"sadness\")\n",
    "df[\"Emotion\"] = df[\"Emotion\"].str.replace(\"optimism\", \"happiness\")\n",
    "df[\"Emotion\"] = df[\"Emotion\"].str.replace(\"admiration\", \"happiness\")\n",
    "df[\"Emotion\"] = df[\"Emotion\"].str.replace(\"approval\", \"happiness\")\n",
    "df[\"Emotion\"] = df[\"Emotion\"].str.replace(\"nervousness\", \"fear\")\n",
    "df[\"Emotion\"] = df[\"Emotion\"].str.replace(\"realization\", \"surprise\")\n",
    "df[\"Emotion\"] = df[\"Emotion\"].str.replace(\"gratitude\", \"happiness\")\n",
    "df[\"Emotion\"] = df[\"Emotion\"].str.replace(\"caring\", \"happiness\")\n",
    "df[\"Emotion\"] = df[\"Emotion\"].str.replace(\"love\", \"happiness\")\n",
    "df[\"Emotion\"] = df[\"Emotion\"].str.replace(\"remorse\", \"sadness\")\n",
    "df[\"Emotion\"] = df[\"Emotion\"].str.replace(\"embarrassment\", \"fear\")\n",
    "df[\"Emotion\"] = df[\"Emotion\"].str.replace(\"grief\", \"sadness\")\n",
    "\n",
    "# Remove unecessary rows that add to the 'happiness' class (there are enough samples)\n",
    "df_test = df[~df['Emotion'].isin(['curiosity', 'desire', 'relief','amusement'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae9a9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Processing data for modeling stage\n",
    "# Encode emotion labels\n",
    "label_encoder_camembert = LabelEncoder()\n",
    "df[\"emotion_label\"] = label_encoder_camembert.fit_transform(df[\"Emotion\"])\n",
    "\n",
    "# Tokenize the sentences using BERT's tokenizer\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "\n",
    "# Modify the function to return a dictionary\n",
    "def encode_sentences(sentences):\n",
    "    return tokenizer(sentences.tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "# Tokenize the sentences\n",
    "encoded_inputs = encode_sentences(df['Sentence'])\n",
    "\n",
    "# Extract input_ids and attention_mask separately\n",
    "input_ids = encoded_inputs[\"input_ids\"]\n",
    "attention_mask = encoded_inputs[\"attention_mask\"]\n",
    "\n",
    "# Now, split data correctly\n",
    "X_train, X_val, attn_train, attn_val, y_train, y_val = train_test_split(\n",
    "    input_ids, attention_mask, df['emotion_label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to tensors\n",
    "train_inputs = torch.tensor(X_train).clone().detach()\n",
    "val_inputs = torch.tensor(X_val).clone().detach()\n",
    "train_attn = torch.tensor(attn_train).clone().detach()\n",
    "val_attn = torch.tensor(attn_val).clone().detach()\n",
    "train_labels = torch.tensor(y_train.values)\n",
    "val_labels = torch.tensor(y_val.values)\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "train_data = TensorDataset(train_inputs, train_attn, train_labels)\n",
    "val_data = TensorDataset(val_inputs, val_attn, val_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168ae52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Model\n",
    "# Load the pre-trained CamemBERT model for sequence classification\n",
    "model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=len(label_encoder_emotion.classes_))\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = AdamW(model.parameters(),lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "# Move model to GPU \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Early Stopping Parameters\n",
    "patience = 3  # Number of epochs with no improvement before stopping\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "num_training_steps = len(train_dataloader) * 10  # Assuming 10 epochs\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/10\", unit=\"batch\", leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "    \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Apply gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix(loss=total_loss / (progress_bar.n + 1), refresh=True)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids, attention_mask, labels = batch  \n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "        \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0  # Reset counter\n",
    "    else:\n",
    "        epochs_no_improve += 1  # Increase counter\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\"Early stopping triggered. Stopping training.\")\n",
    "        break\n",
    "\n",
    "model.save_pretrained(\"./camembert_model_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cba32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Evaluate model\n",
    "# Evaluate the model on the validation set\n",
    "model.eval()\n",
    "val_preds = []\n",
    "val_labels_list = []\n",
    "\n",
    "# Validation loop with tqdm progress bar\n",
    "progress_bar = tqdm(val_dataloader, desc=\"Evaluating\", unit=\"batch\")\n",
    "for batch in progress_bar:\n",
    "    input_ids, attention_mask, labels = batch  \n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "    val_preds.extend(preds.cpu().numpy())\n",
    "    val_labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert the predictions and true labels back to the original class labels\n",
    "val_preds_str = label_encoder_camembert.inverse_transform(val_preds)\n",
    "val_labels_str = label_encoder_camembert.inverse_transform(val_labels_list)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(val_labels_str, val_preds_str)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate F1 Score (Macro Average)\n",
    "f1 = f1_score(val_labels_str, val_preds_str, average='macro')\n",
    "print(f\"F1 Score (Macro Average): {f1}\")\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(val_labels_str, val_preds_str, target_names=label_encoder_camembert.classes_)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(val_labels_str, val_preds_str)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=label_encoder_camembert.classes_, yticklabels=label_encoder_camembert.classes_)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204f4f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on unseen data\n",
    "# Encode the test labels (you don't need to save the label encoder as you're using it in the same notebook)\n",
    "df_test[\"emotion_label\"] = label_encoder_camembert.transform(df_test[\"Emotion\"])\n",
    "\n",
    "# Tokenize the test sentences using the same tokenizer you used for training (CamembertTokenizer)\n",
    "encoded_test_inputs = encode_sentences(df_test['Sentence'])\n",
    "\n",
    "# Extract input_ids and attention_mask for the test set\n",
    "test_input_ids = encoded_test_inputs[\"input_ids\"]\n",
    "test_attention_mask = encoded_test_inputs[\"attention_mask\"]\n",
    "\n",
    "# Convert to tensors\n",
    "test_inputs = torch.tensor(test_input_ids).clone().detach()\n",
    "test_attn = torch.tensor(test_attention_mask).clone().detach()\n",
    "test_labels = torch.tensor(df_test['emotion_label'].values)\n",
    "\n",
    "# Create a DataLoader for batch processing on the test set\n",
    "test_data = TensorDataset(test_inputs, test_attn, test_labels)\n",
    "test_dataloader = DataLoader(test_data, batch_size=16)\n",
    "\n",
    "# The model is already loaded in the notebook, so no need to reload it again\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "test_loss = 0\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "predicted_labels_list = []  # To store all predictions\n",
    "true_labels_list = []      # To store all true labels\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation to save memory\n",
    "    for batch in test_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Get predicted labels (class with the highest logit value)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # Accumulate predictions and true labels\n",
    "        predicted_labels_list.extend(predictions.cpu().numpy())\n",
    "        true_labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Track the number of correct predictions\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "# Convert lists to numpy arrays for metric calculations\n",
    "predicted_labels = np.array(predicted_labels_list)\n",
    "true_labels = np.array(true_labels_list)\n",
    "\n",
    "# Calculate the average test loss and accuracy\n",
    "avg_test_loss = test_loss / len(test_dataloader)\n",
    "accuracy = correct_predictions / total_predictions\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Compute Precision, Recall, F1 Score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5546723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Machine Translation using\n",
    "\n",
    "# Load the dataset\n",
    "sentences_dataset = pd.read_csv('transcribed_data_assemblyAI.csv', sep='\\t', engine='python', encoding='latin1')\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    " \n",
    "# Optimize PyTorch for GPU\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    " \n",
    "# Load model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-fr-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)  # Move model to GPU\n",
    " \n",
    "# Warm-up GPU to avoid initial slow batch\n",
    "dummy_input = torch.tensor([[0]]).to(device)  \n",
    " \n",
    "# Function to translate sentences in batches\n",
    "def translate(sentences, batch_size=8):  # Reduce batch size if stuck\n",
    "    translated = []\n",
    "    for i in tqdm(range(0, len(sentences), batch_size), desc=\"Translating\", unit=\"batch\"):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "       \n",
    "        # Tokenize & move input to GPU\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "       \n",
    "        # Generate translation using GPU\n",
    "        with torch.no_grad():\n",
    "            translated_batch = model.generate(**inputs)\n",
    "       \n",
    "        # Decode output\n",
    "        translated.extend([tokenizer.decode(t, skip_special_tokens=True) for t in translated_batch])\n",
    "   \n",
    "    return translated\n",
    " \n",
    "# Translate sentences from video_1_sentences using GPU\n",
    "sentences_dataset['Translation'] = translate(sentences_dataset['Sentence'].tolist())\n",
    "\n",
    "# Display the translated sentences\n",
    "print(sentences_dataset.head())\n",
    "\n",
    "# Save the translated sentences to a new CSV file\n",
    "sentences_dataset.to_csv('translated_data_assemblyAI.csv', sep='|', index=False)\n",
    "sentences_dataset.to_excel('translated_data_assemblyAI.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Prompt engineering model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blockc_y2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
