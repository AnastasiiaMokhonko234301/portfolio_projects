{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 (Dataset preprocessing for Transformer Usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage: Classifying emotions in transcribed television show data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 12:18:14.424831: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-31 12:18:14.438651: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743423494.453179   15736 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743423494.457614   15736 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743423494.470900   15736 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743423494.470914   15736 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743423494.470916   15736 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743423494.470917   15736 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-31 12:18:14.474855: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO: PyTorch version 2.6.0 available.\n",
      "INFO: TensorFlow version 2.19.0 available.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from transformers import MarianMTModel, MarianTokenizer, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Emotion detection dataset (with phrases from the Friends show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Sentence  \\\n",
      "0  Ce que vous ne comprenez pas, c'est, pour nous...   \n",
      "1              Ouais, c'est vrai! ....... Y'Serious?   \n",
      "2                                          Oh ouais!   \n",
      "3  Tout ce que vous devez savoir est dans ce prem...   \n",
      "4                                        Absolument.   \n",
      "\n",
      "                                          Translated    Emotion  \n",
      "0  What you guys don't understand is, for us, kis...  happiness  \n",
      "1                      Yeah, right!.......Y'serious?    neutral  \n",
      "2                                          Oh, yeah!  happiness  \n",
      "3  Everything you need to know is in that first k...  happiness  \n",
      "4                                        Absolutely.  happiness  \n"
     ]
    }
   ],
   "source": [
    "transcript_2 = \"emory_nlp_ds.csv\"\n",
    "df2 = pd.read_csv(transcript_2)\n",
    "\n",
    "# Switching the column names to match the final dataset\n",
    "df2.rename(columns={'Translated': 'Sentence', 'Sentence': 'Translated'}, inplace=True)\n",
    "df2 = df2[['Sentence'] + [col for col in df2.columns if col != 'Sentence']]\n",
    "\n",
    "# Display first few rows\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding synthetic data to balance classes with the lowest nr of instances (sadness, surprise, and disgust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Sentence  \\\n",
      "0  Je ne peux pas croire que cela vient d'arriver !   \n",
      "1  Je n'avais aucune idée que cela allait arriver !   \n",
      "2                         Wow ! C'était inattendu !   \n",
      "3                       C'est absolument choquant !   \n",
      "4                                 Quelle surprise !   \n",
      "\n",
      "                            Translated   Emotion  \n",
      "0  I can't believe this just happened!  surprise  \n",
      "1       I had no idea this was coming!  surprise  \n",
      "2            Wow! That was unexpected!  surprise  \n",
      "3         This is absolutely shocking!  surprise  \n",
      "4                     What a surprise!  surprise  \n"
     ]
    }
   ],
   "source": [
    "transcript_3 = \"synthetic_emotion_dataset.csv\"\n",
    "df3 = pd.read_csv(transcript_3)\n",
    "\n",
    "# Display first few rows\n",
    "print(df3.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Go Emotions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Sentence  \\\n",
      "0  C'est vrai, mais [NAME] est toujours une resso...   \n",
      "1  Je ne vois pas comment le départ d'un journali...   \n",
      "2  Si les partisans de [NAME] vous déclenchent pl...   \n",
      "3  Et si vous êtes quelqu'un, comme moi, qui croi...   \n",
      "4                     Vous êtes normal, très normal.   \n",
      "\n",
      "                                          Translated    Emotion  \n",
      "0  Thats true but [NAME] is still a valuable reso...   surprise  \n",
      "1  I don't see how a NBC journalist's departure i...      anger  \n",
      "2  If [NAME] supporters trigger you more than mas...    neutral  \n",
      "3  What if you're someone, like me, who believes ...   surprise  \n",
      "4                       You are normal. Very normal.  happiness  \n"
     ]
    }
   ],
   "source": [
    "transcript_3 = \"df_gotranslated.csv\"\n",
    "df4 = pd.read_csv(transcript_3)\n",
    "\n",
    "# Display first few rows\n",
    "print(df4.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatanate the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Translated</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ce que vous ne comprenez pas, c'est, pour nous...</td>\n",
       "      <td>What you guys don't understand is, for us, kis...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ouais, c'est vrai! ....... Y'Serious?</td>\n",
       "      <td>Yeah, right!.......Y'serious?</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oh ouais!</td>\n",
       "      <td>Oh, yeah!</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tout ce que vous devez savoir est dans ce prem...</td>\n",
       "      <td>Everything you need to know is in that first k...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Absolument.</td>\n",
       "      <td>Absolutely.</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58961</th>\n",
       "      <td>Ma scène préférée est la tribu qui ramène le m...</td>\n",
       "      <td>Dozens. My favorite scene is the tribe bringin...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58962</th>\n",
       "      <td>J'aime [NAME], il est littéralement le canapé ...</td>\n",
       "      <td>I love [NAME], he’s literally the craziest cou...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58963</th>\n",
       "      <td>C'est affreux. Je suis content [NAME] de faire...</td>\n",
       "      <td>That's awful. I'm glad [NAME] doing better, bu...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58964</th>\n",
       "      <td>Je vais voir ça.</td>\n",
       "      <td>I'll check that out. Thank you!!</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58965</th>\n",
       "      <td>Votre vidéo ne montre pas comment ?</td>\n",
       "      <td>Your video fails to show how though?</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58966 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence  \\\n",
       "0      Ce que vous ne comprenez pas, c'est, pour nous...   \n",
       "1                  Ouais, c'est vrai! ....... Y'Serious?   \n",
       "2                                              Oh ouais!   \n",
       "3      Tout ce que vous devez savoir est dans ce prem...   \n",
       "4                                            Absolument.   \n",
       "...                                                  ...   \n",
       "58961  Ma scène préférée est la tribu qui ramène le m...   \n",
       "58962  J'aime [NAME], il est littéralement le canapé ...   \n",
       "58963  C'est affreux. Je suis content [NAME] de faire...   \n",
       "58964                                   Je vais voir ça.   \n",
       "58965                Votre vidéo ne montre pas comment ?   \n",
       "\n",
       "                                              Translated    Emotion  \n",
       "0      What you guys don't understand is, for us, kis...  happiness  \n",
       "1                          Yeah, right!.......Y'serious?    neutral  \n",
       "2                                              Oh, yeah!  happiness  \n",
       "3      Everything you need to know is in that first k...  happiness  \n",
       "4                                            Absolutely.  happiness  \n",
       "...                                                  ...        ...  \n",
       "58961  Dozens. My favorite scene is the tribe bringin...  happiness  \n",
       "58962  I love [NAME], he’s literally the craziest cou...  happiness  \n",
       "58963  That's awful. I'm glad [NAME] doing better, bu...    sadness  \n",
       "58964                   I'll check that out. Thank you!!  happiness  \n",
       "58965               Your video fails to show how though?    sadness  \n",
       "\n",
       "[58966 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.concat([df2, df3, df4], ignore_index=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_fr = spacy.load(\"fr_core_news_lg\")\n",
    "def extract_pos(text):\n",
    "    doc = nlp_fr(text)\n",
    "    pos_counts = Counter([token.pos_ for token in doc])\n",
    "    return pos_counts\n",
    "\n",
    "# Function to process the entire dataset and apply the POS extraction\n",
    "def extract_pos_batch(batch):\n",
    "    # Apply the POS extraction for each sentence in the batch\n",
    "    pos_features = [extract_pos(text) for text in batch[\"Sentence\"]]\n",
    "    \n",
    "    # Return the processed POS tags in a suitable format\n",
    "    return {\n",
    "        \"POS_Tags\": [\n",
    "            \" \".join([f\"{key}_{value}\" for key, value in pos.items()])  # Create the POS tag count representation like \"ADP_1 DET_4\"\n",
    "            for pos in pos_features\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR RETAKE\n",
    "\n",
    "nlp_fr = spacy.load(\"fr_core_news_lg\")\n",
    "def extract_pos(text):\n",
    "    doc = nlp_fr(text)\n",
    "    pos_counts = Counter([token.pos_ for token in doc])\n",
    "    total = sum(pos_counts.values())\n",
    "    pos_ratios = {f\"POS_{tag}\": count / total for tag, count in pos_counts.items()}\n",
    "    return pos_ratios\n",
    "\n",
    "def extract_pos_batch(batch):\n",
    "    pos_features = [extract_pos(text) for text in batch[\"Sentence\"]]\n",
    "    return pos_features  \n",
    "\n",
    "## OUTPUT EXAMPLE: \n",
    "\"\"\"{\n",
    "  \"POS_NOUN\": 0.35,\n",
    "  \"POS_VERB\": 0.25,\n",
    "  \"POS_ADJ\": 0.10,\n",
    "  ...\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "## The output is ready to be inputed directly into the model as numeric input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis and Emotion Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the emotion classifier pipeline (for sentiment) and set device to CPU\n",
    "emotion_classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"j-hartmann/emotion-english-distilroberta-base\",  # Specify a model for emotion classification\n",
    "    device=0  # Using GPU 0\n",
    ")\n",
    "\n",
    "# Define get_sentiment and get_intensity functions\n",
    "def get_sentiment(batch):\n",
    "    candidate_labels = [\"happiness\", \"sadness\", \"anger\", \"surprise\", \"fear\", \"disgust\", \"neutral\"]\n",
    "    texts = batch['Sentence']  # Extract all sentences in the batch\n",
    "    results = emotion_classifier(texts, candidate_labels=candidate_labels)  # Process all sentences at once\n",
    "    \n",
    "    # Extract the sentiment (the label with the highest score)\n",
    "    sentiment_scores = [result['labels'][0] for result in results]  # Get the first label (most confident)\n",
    "    return {\"Sentiment_Score\": sentiment_scores}\n",
    "\n",
    "def get_intensity(batch):\n",
    "    texts = batch['Sentence']\n",
    "    results = emotion_classifier(texts, candidate_labels=[\"happiness\", \"anger\", \"fear\", \"sadness\", \"surprise\", \"disgust\"])\n",
    "    \n",
    "    intensities = []\n",
    "    for result in results:\n",
    "        top_score = max(result[\"scores\"])  # Get highest confidence score\n",
    "        \n",
    "        # Define intensity levels based on the score\n",
    "        if top_score < 0.40:\n",
    "            intensities.append(\"Mild\")\n",
    "        elif 0.40 <= top_score < 0.55:\n",
    "            intensities.append(\"Neutral\")\n",
    "        elif 0.55 <= top_score < 0.75:\n",
    "            intensities.append(\"Moderate\")\n",
    "        else:\n",
    "            intensities.append(\"Intense\")\n",
    "    \n",
    "    return {\"Intensity\": intensities}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_fr = spacy.load(\"fr_core_news_lg\")\n",
    "\n",
    "def extract_ner(text):\n",
    "    doc = nlp_fr(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First dataset modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### THIS DATASET HAS BEEN REMOVED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: PyTorch version 2.6.0 available.\n",
      "INFO: TensorFlow version 2.19.0 available.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Use the datasets library for more efficient processing \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(\u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmotion\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Apply the functions to the datasets in batches\u001b[39;00m\n\u001b[1;32m      6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(get_sentiment, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Use the datasets library for more efficient processing \n",
    "dataset = Dataset.from_pandas(df['Emotion'])\n",
    "\n",
    "# Apply the functions to the datasets in batches\n",
    "dataset = dataset.map(get_sentiment, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d1315ab5484eed9aff43fd5270ca09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(get_intensity, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ce86e8f46c424a93925fdb75a5c6d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d49e4838df41f79331eada974ea974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1922762cc24f1a9fc84e6ca5982d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Schema and number of arrays unequal",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Apply Named Entity Recognition (NER) & POS Tags\u001b[39;00m\n\u001b[1;32m     15\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(extract_ner_batch, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_pos_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Apply Word Count & Unique Word Ratio\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_features\u001b[39m(batch):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py:3074\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3070\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3071\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3072\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3073\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3074\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3075\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3076\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py:3531\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3529\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_table(batch\u001b[38;5;241m.\u001b[39mto_arrow())\n\u001b[1;32m   3530\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3531\u001b[0m         \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3532\u001b[0m num_examples_progress_update \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_examples_in_batch\n\u001b[1;32m   3533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m _time \u001b[38;5;241m+\u001b[39m config\u001b[38;5;241m.\u001b[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_writer.py:609\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[0;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[1;32m    607\u001b[0m         inferred_features[col] \u001b[38;5;241m=\u001b[39m typed_sequence\u001b[38;5;241m.\u001b[39mget_inferred_type()\n\u001b[1;32m    608\u001b[0m schema \u001b[38;5;241m=\u001b[39m inferred_features\u001b[38;5;241m.\u001b[39marrow_schema \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\n\u001b[0;32m--> 609\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/table.pxi:4851\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/table.pxi:1603\u001b[0m, in \u001b[0;36mpyarrow.lib._sanitize_arrays\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Schema and number of arrays unequal"
     ]
    }
   ],
   "source": [
    "# Extract punctuation-based features\n",
    "def extract_punctuation_features(batch):\n",
    "    sentences = batch[\"Sentence\"]\n",
    "    \n",
    "    return {\n",
    "        \"Exclamations\": [s.count(\"!\") for s in sentences],\n",
    "        \"Ellipses\": [s.count(\"...\") for s in sentences],\n",
    "        \"Questions\": [s.count(\"?\") for s in sentences]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(extract_punctuation_features, batched=True)\n",
    "\n",
    "\n",
    "# Apply Named Entity Recognition (NER) & POS Tags\n",
    "dataset = dataset.map(extract_ner_batch, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8624dd80a704abb875dcefb24b00867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(extract_pos_batch, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0396924867b47608248143dd562b1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def word_features(batch):\n",
    "    word_lists = [sentence.split() for sentence in batch[\"Sentence\"]]\n",
    "    word_counts = [len(words) for words in word_lists]\n",
    "    unique_word_ratios = [len(set(words)) / len(words) if words else 0 for words in word_lists]\n",
    "\n",
    "    # Convert list of words into a comma-separated string\n",
    "    word_lists_str = [\", \".join(words) for words in word_lists]\n",
    "\n",
    "    return {\"Word_List\": word_lists_str, \"Word_Count\": word_counts, \"Unique_Word_Ratio\": unique_word_ratios}\n",
    "\n",
    "# Apply transformation\n",
    "dataset = dataset.map(word_features, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Translated</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "      <th>Intensity</th>\n",
       "      <th>Exclamations</th>\n",
       "      <th>Ellipses</th>\n",
       "      <th>Questions</th>\n",
       "      <th>NER</th>\n",
       "      <th>Word_List</th>\n",
       "      <th>...</th>\n",
       "      <th>ADV</th>\n",
       "      <th>PRON</th>\n",
       "      <th>DET</th>\n",
       "      <th>ADP</th>\n",
       "      <th>NUM</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>PRT</th>\n",
       "      <th>X</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>POS_Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ce que vous ne comprenez pas, c'est, pour nous...</td>\n",
       "      <td>What you guys don't understand is, for us, kis...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Intense</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Ce, que, vous, ne, comprenez, pas,, c'est,, po...</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>PRON_8 ADV_4 VERB_4 PUNCT_4 ADP_2 SCONJ_1 AUX_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ouais, c'est vrai! ....... Y'Serious?</td>\n",
       "      <td>Yeah, right!.......Y'serious?</td>\n",
       "      <td>neutral</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Intense</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>Ouais,, c'est, vrai!, ......., Y'Serious?</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NOUN_2 PUNCT_4 PRON_1 VERB_1 ADJ_1 X_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oh ouais!</td>\n",
       "      <td>Oh, yeah!</td>\n",
       "      <td>happiness</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Intense</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[Oh ouais!, MISC]]</td>\n",
       "      <td>Oh, ouais!</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NOUN_1 ADJ_1 PUNCT_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tout ce que vous devez savoir est dans ce prem...</td>\n",
       "      <td>Everything you need to know is in that first k...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Intense</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Tout, ce, que, vous, devez, savoir, est, dans,...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ADJ_2 PRON_3 VERB_2 AUX_1 ADP_1 DET_1 NOUN_1 P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Absolument.</td>\n",
       "      <td>Absolutely.</td>\n",
       "      <td>happiness</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Intense</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Absolument.</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ADV_1 PUNCT_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0  Ce que vous ne comprenez pas, c'est, pour nous...   \n",
       "1              Ouais, c'est vrai! ....... Y'Serious?   \n",
       "2                                          Oh ouais!   \n",
       "3  Tout ce que vous devez savoir est dans ce prem...   \n",
       "4                                        Absolument.   \n",
       "\n",
       "                                          Translated    Emotion  \\\n",
       "0  What you guys don't understand is, for us, kis...  happiness   \n",
       "1                      Yeah, right!.......Y'serious?    neutral   \n",
       "2                                          Oh, yeah!  happiness   \n",
       "3  Everything you need to know is in that first k...  happiness   \n",
       "4                                        Absolutely.  happiness   \n",
       "\n",
       "  Sentiment_Score Intensity  Exclamations  Ellipses  Questions  \\\n",
       "0        surprise   Intense             0         0          0   \n",
       "1        surprise   Intense             1         2          1   \n",
       "2        surprise   Intense             1         0          0   \n",
       "3        surprise   Intense             0         0          0   \n",
       "4        surprise   Intense             0         0          0   \n",
       "\n",
       "                   NER                                          Word_List  \\\n",
       "0                   []  Ce, que, vous, ne, comprenez, pas,, c'est,, po...   \n",
       "1                   []          Ouais,, c'est, vrai!, ......., Y'Serious?   \n",
       "2  [[Oh ouais!, MISC]]                                         Oh, ouais!   \n",
       "3                   []  Tout, ce, que, vous, devez, savoir, est, dans,...   \n",
       "4                   []                                        Absolument.   \n",
       "\n",
       "   ...  ADV  PRON  DET  ADP  NUM  CONJ  PRT  X  PUNCT  \\\n",
       "0  ...    4     8    1    2    0     0    0  0      4   \n",
       "1  ...    0     1    0    0    0     0    0  1      4   \n",
       "2  ...    0     0    0    0    0     0    0  0      1   \n",
       "3  ...    0     3    1    1    0     0    0  0      1   \n",
       "4  ...    1     0    0    0    0     0    0  0      1   \n",
       "\n",
       "                                            POS_Tags  \n",
       "0  PRON_8 ADV_4 VERB_4 PUNCT_4 ADP_2 SCONJ_1 AUX_...  \n",
       "1             NOUN_2 PUNCT_4 PRON_1 VERB_1 ADJ_1 X_1  \n",
       "2                               NOUN_1 ADJ_1 PUNCT_1  \n",
       "3  ADJ_2 PRON_3 VERB_2 AUX_1 ADP_1 DET_1 NOUN_1 P...  \n",
       "4                                      ADV_1 PUNCT_1  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert back to Pandas DataFrame\n",
    "df = dataset.to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Translated</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "      <th>Intensity</th>\n",
       "      <th>Exclamations</th>\n",
       "      <th>Ellipses</th>\n",
       "      <th>Questions</th>\n",
       "      <th>NER</th>\n",
       "      <th>Word_List</th>\n",
       "      <th>...</th>\n",
       "      <th>ADV</th>\n",
       "      <th>PRON</th>\n",
       "      <th>DET</th>\n",
       "      <th>ADP</th>\n",
       "      <th>NUM</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>PRT</th>\n",
       "      <th>X</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>POS_Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ce que vous ne comprenez pas, c'est, pour nous...</td>\n",
       "      <td>What you guys don't understand is, for us, kis...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Intense</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Ce, que, vous, ne, comprenez, pas,, c'est,, po...</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>PRON_8 ADV_4 VERB_4 PUNCT_4 ADP_2 SCONJ_1 AUX_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ouais, c'est vrai! ....... Y'Serious?</td>\n",
       "      <td>Yeah, right!.......Y'serious?</td>\n",
       "      <td>neutral</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Intense</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>Ouais,, c'est, vrai!, ......., Y'Serious?</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NOUN_2 PUNCT_4 PRON_1 VERB_1 ADJ_1 X_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oh ouais!</td>\n",
       "      <td>Oh, yeah!</td>\n",
       "      <td>happiness</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Intense</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[Oh ouais!, MISC]]</td>\n",
       "      <td>Oh, ouais!</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NOUN_1 ADJ_1 PUNCT_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tout ce que vous devez savoir est dans ce prem...</td>\n",
       "      <td>Everything you need to know is in that first k...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Intense</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Tout, ce, que, vous, devez, savoir, est, dans,...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ADJ_2 PRON_3 VERB_2 AUX_1 ADP_1 DET_1 NOUN_1 P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Absolument.</td>\n",
       "      <td>Absolutely.</td>\n",
       "      <td>happiness</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Intense</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Absolument.</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ADV_1 PUNCT_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58961</th>\n",
       "      <td>Ma scène préférée est la tribu qui ramène le m...</td>\n",
       "      <td>Dozens. My favorite scene is the tribe bringin...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Intense</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Ma, scène, préférée, est, la, tribu, qui, ramè...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>DET_4 NOUN_4 ADJ_1 AUX_1 PRON_1 VERB_1 ADP_2 P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58962</th>\n",
       "      <td>J'aime [NAME], il est littéralement le canapé ...</td>\n",
       "      <td>I love [NAME], he’s literally the craziest cou...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Intense</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>J'aime, [NAME],, il, est, littéralement, le, c...</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>PRON_3 VERB_4 PUNCT_5 PROPN_1 AUX_2 ADV_3 DET_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58963</th>\n",
       "      <td>C'est affreux. Je suis content [NAME] de faire...</td>\n",
       "      <td>That's awful. I'm glad [NAME] doing better, bu...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Intense</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>C'est, affreux., Je, suis, content, [NAME], de...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>PRON_3 AUX_2 ADJ_2 PUNCT_5 VERB_4 PROPN_1 ADP_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58964</th>\n",
       "      <td>Je vais voir ça.</td>\n",
       "      <td>I'll check that out. Thank you!!</td>\n",
       "      <td>happiness</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Intense</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Je, vais, voir, ça.</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PRON_2 VERB_2 PUNCT_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58965</th>\n",
       "      <td>Votre vidéo ne montre pas comment ?</td>\n",
       "      <td>Your video fails to show how though?</td>\n",
       "      <td>sadness</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Intense</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>Votre, vidéo, ne, montre, pas, comment, ?</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>DET_1 NOUN_1 ADV_3 VERB_1 PUNCT_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58966 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence  \\\n",
       "0      Ce que vous ne comprenez pas, c'est, pour nous...   \n",
       "1                  Ouais, c'est vrai! ....... Y'Serious?   \n",
       "2                                              Oh ouais!   \n",
       "3      Tout ce que vous devez savoir est dans ce prem...   \n",
       "4                                            Absolument.   \n",
       "...                                                  ...   \n",
       "58961  Ma scène préférée est la tribu qui ramène le m...   \n",
       "58962  J'aime [NAME], il est littéralement le canapé ...   \n",
       "58963  C'est affreux. Je suis content [NAME] de faire...   \n",
       "58964                                   Je vais voir ça.   \n",
       "58965                Votre vidéo ne montre pas comment ?   \n",
       "\n",
       "                                              Translated    Emotion  \\\n",
       "0      What you guys don't understand is, for us, kis...  happiness   \n",
       "1                          Yeah, right!.......Y'serious?    neutral   \n",
       "2                                              Oh, yeah!  happiness   \n",
       "3      Everything you need to know is in that first k...  happiness   \n",
       "4                                            Absolutely.  happiness   \n",
       "...                                                  ...        ...   \n",
       "58961  Dozens. My favorite scene is the tribe bringin...  happiness   \n",
       "58962  I love [NAME], he’s literally the craziest cou...  happiness   \n",
       "58963  That's awful. I'm glad [NAME] doing better, bu...    sadness   \n",
       "58964                   I'll check that out. Thank you!!  happiness   \n",
       "58965               Your video fails to show how though?    sadness   \n",
       "\n",
       "      Sentiment_Score Intensity  Exclamations  Ellipses  Questions  \\\n",
       "0            surprise   Intense             0         0          0   \n",
       "1            surprise   Intense             1         2          1   \n",
       "2            surprise   Intense             1         0          0   \n",
       "3            surprise   Intense             0         0          0   \n",
       "4            surprise   Intense             0         0          0   \n",
       "...               ...       ...           ...       ...        ...   \n",
       "58961        surprise   Intense             0         0          0   \n",
       "58962        surprise   Intense             2         0          0   \n",
       "58963        surprise   Intense             0         0          0   \n",
       "58964        surprise   Intense             0         0          0   \n",
       "58965        surprise   Intense             0         0          1   \n",
       "\n",
       "                       NER                                          Word_List  \\\n",
       "0                       []  Ce, que, vous, ne, comprenez, pas,, c'est,, po...   \n",
       "1                       []          Ouais,, c'est, vrai!, ......., Y'Serious?   \n",
       "2      [[Oh ouais!, MISC]]                                         Oh, ouais!   \n",
       "3                       []  Tout, ce, que, vous, devez, savoir, est, dans,...   \n",
       "4                       []                                        Absolument.   \n",
       "...                    ...                                                ...   \n",
       "58961                   []  Ma, scène, préférée, est, la, tribu, qui, ramè...   \n",
       "58962                   []  J'aime, [NAME],, il, est, littéralement, le, c...   \n",
       "58963                   []  C'est, affreux., Je, suis, content, [NAME], de...   \n",
       "58964                   []                                Je, vais, voir, ça.   \n",
       "58965                   []          Votre, vidéo, ne, montre, pas, comment, ?   \n",
       "\n",
       "       ...  ADV  PRON  DET  ADP  NUM  CONJ  PRT  X  PUNCT  \\\n",
       "0      ...    4     8    1    2    0     0    0  0      4   \n",
       "1      ...    0     1    0    0    0     0    0  1      4   \n",
       "2      ...    0     0    0    0    0     0    0  0      1   \n",
       "3      ...    0     3    1    1    0     0    0  0      1   \n",
       "4      ...    1     0    0    0    0     0    0  0      1   \n",
       "...    ...  ...   ...  ...  ...  ...   ...  ... ..    ...   \n",
       "58961  ...    0     1    4    2    0     0    0  0      5   \n",
       "58962  ...    3     3    5    2    0     0    0  0      5   \n",
       "58963  ...    1     3    2    3    0     0    0  0      5   \n",
       "58964  ...    0     2    0    0    0     0    0  0      1   \n",
       "58965  ...    3     0    1    0    0     0    0  0      1   \n",
       "\n",
       "                                                POS_Tags  \n",
       "0      PRON_8 ADV_4 VERB_4 PUNCT_4 ADP_2 SCONJ_1 AUX_...  \n",
       "1                 NOUN_2 PUNCT_4 PRON_1 VERB_1 ADJ_1 X_1  \n",
       "2                                   NOUN_1 ADJ_1 PUNCT_1  \n",
       "3      ADJ_2 PRON_3 VERB_2 AUX_1 ADP_1 DET_1 NOUN_1 P...  \n",
       "4                                          ADV_1 PUNCT_1  \n",
       "...                                                  ...  \n",
       "58961  DET_4 NOUN_4 ADJ_1 AUX_1 PRON_1 VERB_1 ADP_2 P...  \n",
       "58962  PRON_3 VERB_4 PUNCT_5 PROPN_1 AUX_2 ADV_3 DET_...  \n",
       "58963  PRON_3 AUX_2 ADJ_2 PUNCT_5 VERB_4 PROPN_1 ADP_...  \n",
       "58964                              PRON_2 VERB_2 PUNCT_1  \n",
       "58965                  DET_1 NOUN_1 ADV_3 VERB_1 PUNCT_1  \n",
       "\n",
       "[58966 rows x 25 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(2, 5), stop_words=None)\n",
    "\n",
    "# Compute TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(df[\"Sentence\"].astype(str))\n",
    "\n",
    "# Convert TF-IDF values to a list for each row\n",
    "df[\"TF-IDF\"] = tfidf_matrix.toarray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"ADJ\", \"ADP\", \"ADV\", \"CONJ\", \"DET\", \n",
    "    \"NOUN\", \"NUM\", \"PRON\", \"PUNCT\", \"VERB\", \"X\", \"PRT\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 58966 entries, 0 to 58965\n",
      "Data columns (total 14 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Sentence           58966 non-null  object \n",
      " 1   Translated         58966 non-null  object \n",
      " 2   Emotion            58966 non-null  object \n",
      " 3   Sentiment_Score    58966 non-null  object \n",
      " 4   Intensity          58966 non-null  object \n",
      " 5   Exclamations       58966 non-null  int64  \n",
      " 6   Ellipses           58966 non-null  int64  \n",
      " 7   Questions          58966 non-null  int64  \n",
      " 8   NER                58966 non-null  object \n",
      " 9   Word_List          58966 non-null  object \n",
      " 10  Word_Count         58966 non-null  int64  \n",
      " 11  Unique_Word_Ratio  58966 non-null  float64\n",
      " 12  POS_Tags           58966 non-null  object \n",
      " 13  TF-IDF             58966 non-null  object \n",
      "dtypes: float64(1), int64(4), object(9)\n",
      "memory usage: 6.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Translated</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "      <th>Intensity</th>\n",
       "      <th>Exclamations</th>\n",
       "      <th>Ellipses</th>\n",
       "      <th>Questions</th>\n",
       "      <th>NER</th>\n",
       "      <th>Word_List</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>Unique_Word_Ratio</th>\n",
       "      <th>POS_Tags</th>\n",
       "      <th>TF-IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ce que vous ne comprenez pas, c'est, pour nous...</td>\n",
       "      <td>What you guys don't understand is, for us, kis...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Intense</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Ce, que, vous, ne, comprenez, pas,, c'est,, po...</td>\n",
       "      <td>21</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>PRON_8 ADV_4 VERB_4 PUNCT_4 ADP_2 SCONJ_1 AUX_...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ouais, c'est vrai! ....... Y'Serious?</td>\n",
       "      <td>Yeah, right!.......Y'serious?</td>\n",
       "      <td>neutral</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Intense</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>Ouais,, c'est, vrai!, ......., Y'Serious?</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NOUN_2 PUNCT_4 PRON_1 VERB_1 ADJ_1 X_1</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oh ouais!</td>\n",
       "      <td>Oh, yeah!</td>\n",
       "      <td>happiness</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Intense</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[Oh ouais!, MISC]]</td>\n",
       "      <td>Oh, ouais!</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NOUN_1 ADJ_1 PUNCT_1</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tout ce que vous devez savoir est dans ce prem...</td>\n",
       "      <td>Everything you need to know is in that first k...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Intense</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Tout, ce, que, vous, devez, savoir, est, dans,...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>ADJ_2 PRON_3 VERB_2 AUX_1 ADP_1 DET_1 NOUN_1 P...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Absolument.</td>\n",
       "      <td>Absolutely.</td>\n",
       "      <td>happiness</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Intense</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Absolument.</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>ADV_1 PUNCT_1</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58961</th>\n",
       "      <td>Ma scène préférée est la tribu qui ramène le m...</td>\n",
       "      <td>Dozens. My favorite scene is the tribe bringin...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Intense</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Ma, scène, préférée, est, la, tribu, qui, ramè...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>DET_4 NOUN_4 ADJ_1 AUX_1 PRON_1 VERB_1 ADP_2 P...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58962</th>\n",
       "      <td>J'aime [NAME], il est littéralement le canapé ...</td>\n",
       "      <td>I love [NAME], he’s literally the craziest cou...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Intense</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>J'aime, [NAME],, il, est, littéralement, le, c...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>PRON_3 VERB_4 PUNCT_5 PROPN_1 AUX_2 ADV_3 DET_...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58963</th>\n",
       "      <td>C'est affreux. Je suis content [NAME] de faire...</td>\n",
       "      <td>That's awful. I'm glad [NAME] doing better, bu...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Intense</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>C'est, affreux., Je, suis, content, [NAME], de...</td>\n",
       "      <td>21</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>PRON_3 AUX_2 ADJ_2 PUNCT_5 VERB_4 PROPN_1 ADP_...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58964</th>\n",
       "      <td>Je vais voir ça.</td>\n",
       "      <td>I'll check that out. Thank you!!</td>\n",
       "      <td>happiness</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Intense</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Je, vais, voir, ça.</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>PRON_2 VERB_2 PUNCT_1</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58965</th>\n",
       "      <td>Votre vidéo ne montre pas comment ?</td>\n",
       "      <td>Your video fails to show how though?</td>\n",
       "      <td>sadness</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Intense</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>Votre, vidéo, ne, montre, pas, comment, ?</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>DET_1 NOUN_1 ADV_3 VERB_1 PUNCT_1</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57568 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence  \\\n",
       "0      Ce que vous ne comprenez pas, c'est, pour nous...   \n",
       "1                  Ouais, c'est vrai! ....... Y'Serious?   \n",
       "2                                              Oh ouais!   \n",
       "3      Tout ce que vous devez savoir est dans ce prem...   \n",
       "4                                            Absolument.   \n",
       "...                                                  ...   \n",
       "58961  Ma scène préférée est la tribu qui ramène le m...   \n",
       "58962  J'aime [NAME], il est littéralement le canapé ...   \n",
       "58963  C'est affreux. Je suis content [NAME] de faire...   \n",
       "58964                                   Je vais voir ça.   \n",
       "58965                Votre vidéo ne montre pas comment ?   \n",
       "\n",
       "                                              Translated    Emotion  \\\n",
       "0      What you guys don't understand is, for us, kis...  happiness   \n",
       "1                          Yeah, right!.......Y'serious?    neutral   \n",
       "2                                              Oh, yeah!  happiness   \n",
       "3      Everything you need to know is in that first k...  happiness   \n",
       "4                                            Absolutely.  happiness   \n",
       "...                                                  ...        ...   \n",
       "58961  Dozens. My favorite scene is the tribe bringin...  happiness   \n",
       "58962  I love [NAME], he’s literally the craziest cou...  happiness   \n",
       "58963  That's awful. I'm glad [NAME] doing better, bu...    sadness   \n",
       "58964                   I'll check that out. Thank you!!  happiness   \n",
       "58965               Your video fails to show how though?    sadness   \n",
       "\n",
       "      Sentiment_Score Intensity  Exclamations  Ellipses  Questions  \\\n",
       "0            surprise   Intense             0         0          0   \n",
       "1            surprise   Intense             1         2          1   \n",
       "2            surprise   Intense             1         0          0   \n",
       "3            surprise   Intense             0         0          0   \n",
       "4            surprise   Intense             0         0          0   \n",
       "...               ...       ...           ...       ...        ...   \n",
       "58961        surprise   Intense             0         0          0   \n",
       "58962        surprise   Intense             2         0          0   \n",
       "58963        surprise   Intense             0         0          0   \n",
       "58964        surprise   Intense             0         0          0   \n",
       "58965        surprise   Intense             0         0          1   \n",
       "\n",
       "                       NER                                          Word_List  \\\n",
       "0                       []  Ce, que, vous, ne, comprenez, pas,, c'est,, po...   \n",
       "1                       []          Ouais,, c'est, vrai!, ......., Y'Serious?   \n",
       "2      [[Oh ouais!, MISC]]                                         Oh, ouais!   \n",
       "3                       []  Tout, ce, que, vous, devez, savoir, est, dans,...   \n",
       "4                       []                                        Absolument.   \n",
       "...                    ...                                                ...   \n",
       "58961                   []  Ma, scène, préférée, est, la, tribu, qui, ramè...   \n",
       "58962                   []  J'aime, [NAME],, il, est, littéralement, le, c...   \n",
       "58963                   []  C'est, affreux., Je, suis, content, [NAME], de...   \n",
       "58964                   []                                Je, vais, voir, ça.   \n",
       "58965                   []          Votre, vidéo, ne, montre, pas, comment, ?   \n",
       "\n",
       "       Word_Count  Unique_Word_Ratio  \\\n",
       "0              21           0.904762   \n",
       "1               5           1.000000   \n",
       "2               2           1.000000   \n",
       "3              11           0.909091   \n",
       "4               1           1.000000   \n",
       "...           ...                ...   \n",
       "58961          17           0.823529   \n",
       "58962          26           0.884615   \n",
       "58963          21           0.904762   \n",
       "58964           4           1.000000   \n",
       "58965           7           1.000000   \n",
       "\n",
       "                                                POS_Tags  \\\n",
       "0      PRON_8 ADV_4 VERB_4 PUNCT_4 ADP_2 SCONJ_1 AUX_...   \n",
       "1                 NOUN_2 PUNCT_4 PRON_1 VERB_1 ADJ_1 X_1   \n",
       "2                                   NOUN_1 ADJ_1 PUNCT_1   \n",
       "3      ADJ_2 PRON_3 VERB_2 AUX_1 ADP_1 DET_1 NOUN_1 P...   \n",
       "4                                          ADV_1 PUNCT_1   \n",
       "...                                                  ...   \n",
       "58961  DET_4 NOUN_4 ADJ_1 AUX_1 PRON_1 VERB_1 ADP_2 P...   \n",
       "58962  PRON_3 VERB_4 PUNCT_5 PROPN_1 AUX_2 ADV_3 DET_...   \n",
       "58963  PRON_3 AUX_2 ADJ_2 PUNCT_5 VERB_4 PROPN_1 ADP_...   \n",
       "58964                              PRON_2 VERB_2 PUNCT_1   \n",
       "58965                  DET_1 NOUN_1 ADV_3 VERB_1 PUNCT_1   \n",
       "\n",
       "                                                  TF-IDF  \n",
       "0      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "...                                                  ...  \n",
       "58961  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "58962  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "58963  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "58964  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "58965  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "\n",
       "[57568 rows x 14 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Intensity\"] == \"Intense\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataset\n",
    "cleaned_file_path = \"../Task 4/transformers_dataset.csv\"\n",
    "df.to_csv(cleaned_file_path, index=False, encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
